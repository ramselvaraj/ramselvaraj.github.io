---
title: 'Building a Neural Network from scratch in C'
date: 2024-10-29
permalink: /posts/2024/10/neural-cee/
tags:
  - programming
  - deep learning
  - machine learning
  - c
---

Building a binary image classifier from scratch and training it entirely in C.

Firstly, what is a Neural Network?
======
A neural network is a deep learning model in which an input passes through 'layers', while they get transformed by being multiplied with 'weights' and have 'biases' added to them. 'Layers' consist of 'nodes', which in turn contains the 'weights', so a layer can have multiple nodes depending on the required dimensionality we are working with.

The network can be trained to perform the required tasks by adjusting these weights, with the help of gradient descent and backward propogation. The basic idea being that with the training data, we will go through the network first and then make a backward sweep adjusting the weights depending on deviation of our prediction from the target output.

Why in C?
======
I have chosen C for the purpose of having as less of an abstraction as possible when it comes to implementing the network and training it with backward propogation, such as PyTorch or even using the numpy module. Along with the standard libraries, only an image processing library is being used(given that it is not the task at hand).

Binary Image Classifier 
======
The task chosen here is to build a binary image classifier, i.e a model that can classify whether an image belongs to one of two possible classes. The goal is to predict whether an image is that of a cat or a dog.

Dataset
------
The dataset chosen here is a set of labelled cat and dog images. Our target labels are Cat(1) and Dog(0).

Network Architecture
------
As seen in the image there is only one layer in this network. Traditionally image related tasks utilise CNNs, but for this experiment a simple 1x128x128 layer is being used, with corresponding weights for each pixel of the input image. The output of the network will have a sigmoid activation function applied to have the value b/w 0 and 1. This way, regardless of the actual pixel values which can give very large outputs after the weights are applied, we will always have a value between 0 and 1 which can be used to compare it with out target labels.

The weights in the beginning will be randomly initialised, with the idea being we can adjust their values during training.

How does the network get trained?
======
With the network architecture decided, it can be implemented and forward propogation of data can happen, but it will yield random predictions as it hasn't been trained yet. To train the network we will make use of gradient descent and backward propogation.

The first step is to take our prediction and compare it with the target label, let us assume that this is computed as a cost function. The idea is then to minimize the value of this cost function, which would mean that the prediction is as close to the target value as possible. The point at which the cost function will return the minimum value is calculated using gradient descent, where we take the differential(gradient) of the cost function.

The next step is to take this computed gradient and use it to adjust the weights somehow, which is where the actual 'learning' happens. This is where backward propogation comes in. Backward propogation involves making a backward sweep all the way back to the first layer and adjusting the weights with the computed gradients for each layer. The gradient is usually multiplied with a 'learning rate' parameter before being used to adjust the weight.

This process is then repeated for all instances in the training set, for a number of times (calcuated as 'epochs'), thus attempting to correct and calculate a generalized value for the weights from their original randomly initialised values. The generalized values will be able to perform the required task across the entire dataset.

Programming Implemention
======
Loading and using the dataset in C-
------

Neural Network in C-
------
### Architecture
### Feed Forwarad
### Loss Function
### Gradient Descent
### Backward Propogation
### Training and Learning Functions

