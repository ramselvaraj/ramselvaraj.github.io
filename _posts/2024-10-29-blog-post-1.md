---
title: 'Building a Neural Network from scratch in C'
date: 2024-10-29
permalink: /posts/2024/10/neural-cee/
tags:
  - programming
  - deep learning
  - machine learning
  - c
---

[GitHub Repo](https://github.com/ramselvaraj/neural-cee/)

Building a binary image classifier from scratch and training it in C.

![Diagram](/files/images/2024_10_29_neural_cee_1.png)

Firstly, what is a Neural Network?
======
A neural network is a deep learning model in which an input passes through 'layers', while they get transformed by being multiplied with 'weights' and have 'biases' added to them. 'Layers' consist of 'nodes', which in turn contains the 'weights', so a layer can have multiple nodes depending on the required dimensionality we are working with.

The network can be trained to perform the required tasks by adjusting these weights, with the help of gradient descent and backward propogation. The basic idea being that with the training data, we will go through the network first and then make a backward sweep adjusting the weights depending on deviation of our prediction from the target output.

Why in C?
======
I have chosen C for the purpose of having as less of an abstraction as possible when it comes to implementing the network and training it with backward propogation, such as PyTorch or even using the numpy module. Along with the standard libraries, only an image processing library is being used(given that it is not the task at hand).

Binary Image Classifier 
======
The task chosen here is to build a binary image classifier, i.e a model that can classify whether an image belongs to one of two possible classes. The goal is to predict whether an image is that of a cat or a dog.

Dataset
------
The dataset chosen here is a set of labelled cat and dog images. Our target labels are Cat(1) and Dog(0).

Network Architecture
------
As seen in the image there is only one layer in this network. Traditionally image related tasks utilise CNNs, but for this experiment a simple 1x128x128 layer is being used, with corresponding weights for each pixel of the input image. The output of the network will have a sigmoid activation function applied to have the value b/w 0 and 1. This way, regardless of the actual pixel values which can give very large outputs after the weights are applied, we will always have a value between 0 and 1 which can be used to compare it with out target labels.

The weights in the beginning will be randomly initialised, with the idea being we can adjust their values during training.

How does the network get trained?
======
With the network architecture decided, it can be implemented and forward propogation of data can happen, but it will yield random predictions as it hasn't been trained yet. To train the network we will make use of gradient descent and backward propogation.

The first step is to take our prediction and compare it with the target label, let us assume that this is computed as a cost function. The idea is then to minimize the value of this cost function, which would mean that the prediction is as close to the target value as possible. The point at which the cost function will return the minimum value is calculated using gradient descent, where we take the differential(gradient) of the cost function.

The next step is to take this computed gradient and use it to adjust the weights somehow, which is where the actual 'learning' happens. This is where backward propogation comes in. Backward propogation involves making a backward sweep all the way back to the first layer and adjusting the weights with the computed gradients for each layer. The gradient is usually multiplied with a 'learning rate' parameter before being used to adjust the weight.

This process is then repeated for all instances in the training set, for a number of times (calcuated as 'epochs'), thus attempting to correct and calculate a generalized value for the weights from their original randomly initialised values. The generalized values will be able to perform the required task across the entire dataset.

Programming Implemention
======
This section will discuss how it is implemented in C

Loading and using the dataset in C
------
The first task is to load the image data into a format that can be used as input to the neural network. This involves loading the pixel information in float format, and to facilitate this, the [nothings stb](https://github.com/nothings/stb) is being used.

With the help of this library, we can load the image data in float arrays. The image data is captured and stored in an array of size of Width x Height x Channel, as those are the number of pixels present in the image. Also, we can resize the images to our desired width and height (128x128), which will be the dimensions of the neural network layer as well. 
~~~ c
void load_image(const char *filename, float *image){
  int width, height, channels;

  float * data = stbi_loadf(filename, &width, &height, &channels, IMAGE_CHANNELS);

  stbir_resize_float_linear(data, width, height, 0, image, IMAGE_WIDTH, IMAGE_HEIGHT, 0, STBIR_RGB);

  stbi_image_free(data);
}

void load_dataset(const char *path, const char *label, int count, float train[][IMAGE_WIDTH*IMAGE_HEIGHT*IMAGE_CHANNELS]){
  for (int i = 0; i < count; i++){
    char filename[128] = {0};
    sprintf(filename, "%s/%s.%d.jpg", path, label, i);
    load_image(filename, train[i]);
  } 
}
~~~

Neural Network in C-
------
### Architecture
The neural network is a struct with weights and biases. The weights are a single float array, and are in the same dimensions as per our design. So it can be visualized that the weights array used here is basically the layer in our neural network. The struct also has additional data points, which will hold the values of the intermediate calculations, which are helpful for calculating the gradient descent as will be seen in the further sections.

The init function simply initialises the weights of the network with random values, but with a custom function that will ensure the weights are always within the range of -1 to 1.
~~~ c
typedef struct neural_network{
  // 0 refers to the previous feed forward, 1 is the current
  float a_0[IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS];
  float w_1[IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS];
  float a_1;
  float z_1;
  float b_1;
} neural_network;

float rand11(){
  return (rand() / (float)RAND_MAX - 0.5) * 2.0;
}

void nn_init(neural_network *nn){
  for(int i = 0; i < IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS; i++){
    nn->w_1[i] = rand11();
  }
  nn->b_1 = rand11();
}
~~~

| Parameter | Purpose |
| a_0 | Stores the value of the input image, used in gradient calculation |
| w_1 | Stores the weights for the specified layer |
| z_1 | Stores the value of the dot_product b/w the input and weights array |
| a_1 | Stores the value of z_1 passed through the sigmoid activation function |
| b_1 | Stores the value of the bias | 

### Feed Forwarad
This is a function that will take the input image data and send it through the neural network, i.e perform all the operations required on the input data as it moves through the network.
This includes performing the dot product b/w the input data and weights, then adding it with the bias, followed by the sigmoid activation function, which is the final returned value.
Crucially, also at the beginning, the parameter a_0 is also filled, which will again be used while computing the gradient.

~~~ c
float nn_forward(neural_network *nn, float *x){
  memcpy(nn->a_0, x, IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS * sizeof(float));
  nn->z_1 = dot_product(nn->w_1, x, IMAGE_WIDTH * IMAGE_HEIGHT * IMAGE_CHANNELS) + nn->b_1;
  nn->a_1 = sigmoid(nn->z_1);
  return nn->a_1;
}
~~~

### Loss Function & Gradient Descent

~~~ c

~~~

### Backward Propogation

~~~ c

~~~

### Training and Learning Functions

~~~ c

~~~
